---
---

@string{aps = {American Physical Society,}}

@article{openrag,
  abbr={Open-RAG},
  title={Open-RAG: Enhanced Retrieval Augmented Reasoning with Open-Source
Large Language Models},
  author={Bin Islam, Shayekh and Rahman, Md Asib and Tozammel Hossain, K S M and Hoque, Enamul and Joty, Shafiq, and Parvez, Md Rizwan},
  abstract={Retrieval Augmented Generation (RAG) has been shown to enhance the factual accuracy of Large Language Models (LLMs) by providing external evidence, but existing methods often suffer from limited reasoning capabilities (e.g., multi-hop complexities) in effectively using such evidence, particularly when using open-source LLMs. To mitigate this gap, in this paper, we introduce a novel framework, Open-RAG, designed to enhance reasoning capabilities in RAG with open-source LLMs. Our framework transforms an arbitrary dense LLM into a parameter-efficient sparse mixture of experts (MoE) model capable of handling complex reasoning tasks, including both single- and multi-hop queries. Open-RAG uniquely trains the model to navigate challenging distractors that appear relevant but are misleading. By combining the constructive learning and architectural transformation, Open-RAG leverages latent learning, dynamically selecting relevant experts and integrating external knowledge effectively for more accurate and contextually relevant responses. Additionally, we propose a hybrid adaptive retrieval method to determine retrieval necessity and balance the trade-off between performance gain and inference speed. Experimental results show that Open-RAG outperforms state-of-the-art LLMs and RAG models in various knowledge-intensive tasks. Our method based on Llama2 sets new benchmarks, surpassing ChatGPT-RAG, Command R+ and Self-RAG.},
  year={2024},
  month={August},
  pdf={example_pdf.pdf},
  dimensions={true},
  selected={true}
}

